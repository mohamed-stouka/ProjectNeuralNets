{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Loss Function Optimization for Recurrent Neural Networks </center>\n",
    "\n",
    "<center>By \"Mohamed Stouka\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<img src=\"https://news.mit.edu/sites/default/files/styles/news_article__image_gallery/public/images/202011/MIT-Network-Confidence-01-Press_0.jpg?itok=Wnt9cX6G\" width=\"20%\">\n",
    "\n",
    "Image from: https://news.mit.edu/2020/neural-network-uncertainty-1120 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Overview\n",
    "\n",
    "Generally, when building a machine learning model, some questions that usually comes into mind: How is a model being optimized? Why does Model A outperform Model B? One of the key points to answer these questions is understanding loss functions of different models, and furthermore, being able to choose an appropriate loss function or self-define a loss function based on the goal of the project and the tolerance of error type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Program Description\n",
    "\n",
    "For Recurrent Neural Networks, models are optimized by finding optimal coefficients that minimize cost function. Cost function is the sum of losses from each data point calculated with loss function. Cost function is a parabola curve. To minimize it, we need to find its vertex. It can be solved analytically or by using programming algorithms. In this project, I will try to accomplish this by one of the most popular programming solutions, Gradient Descent, or by using optimization tools provided from python libraries such as Scipy.optimize. In this project, the dummy dataset that will be used to test the results of the  recurrent neural network will be ur with the MNIST data set for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Project Goals and Timeline\n",
    "\n",
    "- short term: research about optimization techniques and algorithms\n",
    "- mid term: experiment the optimization techniques on built-in loss functions and self-made loss functions\n",
    "- long term: try the code on different types of RNNs and compare results\n",
    "\n",
    "\n",
    "\n",
    "- 1/29/2021 - Create git repository \n",
    "- 2/12/2021 - Proposal Due\n",
    "- 2/26/2021 - Stub functions and Example code integration (With documentation)\n",
    "- 3/12/2021 - Unit Test Integration\n",
    "- 3/25/2021 - Coding Standards and Linting\n",
    "- 4/9/2021 - Code Review \n",
    "- 4/16/2021 - Presentation Video Due\n",
    "- 4/23/2021 - Final Report and Code due.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Anticipating Challenges  \n",
    "\n",
    "challanges will include understanding the algebra behind existing loss functions, also, learn about different backends to build the model on, for example, tensorflow and theano, and understand advantages and dissadvanages of both.Futhrurmore, learn about python optimization libraries and how to implement them on neaural networks."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
